\documentclass[sigconf, authoryear]{acmart}

\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{titlesec}


\titlespacing*{\subsubsection}{0pt}{*0}{1ex}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
    \providecommand\BibTeX{{%
        Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
conference title from your rights confirmation email}{June 03--05,
    2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{LLM-as-a-judge}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Mokrov Semen}
\affiliation{%
    \institution{Amsterdam University of Applied Science}
    \city{Amsterdam}
    \country{The Netherlands}}
\email{semen.mokrov@gmail.com}

\author{Deep Kayal}
\affiliation{%
    \institution{Amazon Research}
    \city{London}
    \country{The United Kingdom}}
\email{_}

\author{Riccardo Pinosio}
\affiliation{%
    \institution{Amsterdam University of Applied Science}
    \city{Amsterdam}
    \country{The Netherlands}}
\email{_}

\author{Debarati Bhaumik}
\affiliation{%
    \institution{Amsterdam University of Applied Science}
    \city{Amsterdam}
    \country{The Netherlands}}
\email{_}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Mokrov et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    Here the abstract
\end{abstract}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Here the introduction


\section{Related Work}\label{sec:related-work}

\subsection{Large Language Models as Evaluators}\label{subsec:large-language-models-as-evaluators}

\subsubsection{Introduction to LLM Evaluators}

Large Language Models (LLMs) are increasingly being used as automated evaluators for open-ended text generation tasks, providing a scalable alternative to slow and costly human judgment.
Recent benchmarks and leaderboards for chatbots and text generation often rely on powerful LLMs (such as GPT-4) to rank or score model outputs in place of humans.
For instance, the Vicuna Chatbot evaluation pipeline employs GPT-4 to compare responses in a pairwise manner, achieving evaluation agreements on par with human annotators on many queries.
Researchers have even built interactive frameworks like \textit{EvalGen} \cite{evalgen} that leverage LLMs to suggest evaluation criteria and generate test queries for model outputs, with a human-in-the-loop to refine these criteria.
These approaches demonstrated the potential of LLM-as-a-judge systems: a strong LLM can approximate human evaluation reasonably well (e.g., GPT-4’s judgments agree with human preferences roughly 75–80\% of the time on multi-turn dialog tasks) \cite{mtbench}.
As a result, LLM evaluators have quickly become a central tool for comparing models and tuning them to human-like performance.

\subsubsection{Evaluation Paradigms}


LLM-as-a-Judge evaluations mostly employ two paradigms: pointwise and pairwise evaluations.


\paragraph{Pointwise Evaluation}


In pointwise evaluation, an LLM assesses a single response independently, assigning a score based on predefined criteria such as relevance, coherence, or factual accuracy.
This method is straightforward and allows for fine-grained analysis of individual responses.
However, it may suffer from inconsistencies due to the subjective nature of scoring and the lack of comparative context \cite{evalgen}.


\paragraph{Pairwise Evaluation}


Pairwise evaluation involves presenting the LLM with two responses to the same prompt and asking it to determine which one is better according to specific criteria \cite{liusie2024llmcomparativeassessmentzeroshot}.
This comparative approach can mitigate some biases inherent in pointwise evaluations and often aligns more closely with human judgment.
In score-based pairwise comparison, the LLM assigns each response a numerical score based on evaluation criteria such as coherence, helpfulness, or relevance, and then ranks them by comparing the scores \cite{liu2024aligning}.
This method allows for fine-grained differentiation between responses while retaining the benefits of comparative judgment.

\subsubsection{Benchmarks Utilizing LLM-as-a-Judge}


Several benchmarks have been developed to standardize the evaluation of LLM outputs using the LLM-as-a-Judge paradigm.


\paragraph{MT-Bench}


MT-Bench is a multi-turn benchmark designed to assess the conversational abilities of LLMs.
It evaluates models on their capacity to maintain context, follow instructions, and provide informative responses over multiple dialogue turns.
MT-Bench employs GPT-4 as the judge, leveraging its advanced understanding to score model responses.
The benchmark includes an Elo rating system derived from pairwise comparisons, offering a dynamic leaderboard that reflects the relative performance of various models\cite{mtbench}.


\paragraph{AlpacaEval}


AlpacaEval focuses on single-turn instruction-following tasks, evaluating models based on their ability to generate helpful and accurate responses.
It uses GPT-4 as an automatic evaluator to compare model outputs against reference responses.
AlpacaEval provides win-rate statistics and supports a leaderboard that ranks models accordingly.
The benchmark has been validated against a substantial set of human annotations, ensuring its reliability and relevance\cite{alpaca_eval}.


\paragraph{Chatbot Arena}


Chatbot Arena is a platform that combines human and LLM-based evaluations to rank conversational agents.
It utilizes pairwise comparisons, where human annotators or LLM judges select the better response between two options.
The results contribute to an Elo rating system, providing a comprehensive leaderboard that reflects both human and automated assessments.
Chatbot Arena serves as a valuable resource for understanding the comparative performance of various chatbots in real-world scenarios\cite{mtbench}.


\subsection{Problems in Current Systems}\label{subsec:problems-in-current-systems}

\subsubsection{Position Bias}

Position bias refers to the tendency of LLM-based evaluators to favor responses based on their position in a prompt during pairwise comparisons.
Studies have shown that models like GPT-4 often prefer the first response presented, regardless of content quality\cite{shi2024judging}.
This bias can compromise the fairness and reliability of evaluations.
To address this, frameworks such as PORTIA have been developed, which align similar content across candidate answers to mitigate position bias effectively\cite{li-etal-2024-split}.

\subsubsection{Length Bias}

Length bias occurs when LLM evaluators disproportionately favor longer responses, associating verbosity with higher quality.
This can lead to inflated evaluations for unnecessarily lengthy outputs.
Research has demonstrated that longer responses often receive higher preference scores, even when shorter responses are equally or more informative\cite{hu2024explaining}.

\subsubsection{Cost Problem}

Employing state-of-the-art large language models (LLMs) as evaluators in LLM-as-a-Judge frameworks introduces significant computational costs.
High-performance models like GPT-4 entail substantial expenses per token, rendering large-scale evaluations financially heavy.
Simpler queries are handled by smaller, cost-effective models, while more complex tasks are escalated to larger, more expensive models.
This strategy can significantly reduce overall evaluation costs without compromising accuracy\cite{chen2023frugalgpt}.
At the same time, ARJudge optimizes the trade-off between evaluation quality and computational expense by dynamically allocating resources based on the prompt characteristics\cite{ar_judge}.

\subsection{Established Techniques for Biases Mitigation}\label{subsec:established-techniques-for-bias-mitigation}

This chapter presents established techniques for mitigating position bias, length bias, and cost-related issues.


\textbf{PORTIA} (Position-Oriented Response Transformation and Input Alignment) is an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner.
Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs.
Experiments demonstrate that PORTIA markedly enhances the consistency rates for various models and comparison forms, achieving an average relative improvement of 47.46\%.
Furthermore, it rectifies around 80\% of the position bias instances within the GPT-4 model, elevating its consistency rate up to 98\%.
However, PORTIA is not effective when the LLM evaluator refuses to provide a judgment, often seen in advanced models like GPT-3.5 and GPT-4 in sensitive domains (e.g., roleplay tasks).
Additionally, it requires the merged input to fit within the model’s context window, and it's not reproducible since it doesn't provide a working codebase at the time of writing\cite{portia}.

\textbf{FairEval} introduces a calibration framework to address positional bias in LLM evaluations.
Extensive experiments demonstrate that FairEval successfully alleviates evaluation bias, achieving a closer alignment with human judgment by improving consistency and reducing positional artifacts in comparative outputs.
Specifically, the combined calibration strategy using all the components enhances the accuracy of GPT-4 and ChatGPT’s evaluations by up to 14.3\% and 26.9\%, respectively, with a kappa correlation improvement of 0.25 and 0.46.
However, despite its effectiveness, FairEval's reliance on multiple API calls and partial human intervention may lead to increased computational costs and scalability limitations when applied to large-scale model comparisons\cite{faireval}.


\textbf{LC AlpacaEval} addresses length bias issue by training a logistic regression model on prior annotations from AlpacaEval, factoring in model identity, prompt length, and instruction difficulty.
By neutralizing the response length component during score prediction, this method improves evaluation fairness.
As a result, the automatic evaluation match human opinions better (raising the Spearman correlation with Chatbot Arena from 0.94 to 0.98).
Moreover, their system is more robust to the fluctuations in the win rate: before, models could change their win rate by as much as 41.4 percentage points just by changing how long their answers were; now, it is 9.7.
Also, during adversarial attack (attempt to deceive the system by purpose), the win rate only goes up by 8.5 points, instead of 22.2.
However, LC AlpacaEval's performance is bounded by the scope of its training data; it struggles to generalize effectively to more complex multi-turn tasks and open-ended domains, where human judgment nuances are more difficult to approximate\cite{lc_alpacha}.


\textbf{CascadeEval} proposes a cascade-based evaluation mechanism with early abstention, allowing smaller, cheaper LLMs to abstain from answering difficult queries and defer them to stronger models.
This leads to a 13.0\% average reduction in cost and a 5.0\% drop in error rates across six benchmarks, with only a 4.1\% increase in abstention rates, demonstrating strong performance-cost trade-offs.
However, the method requires careful calibration of confidence thresholds and may depend on benchmark-specific tuning\cite{cascade_eval}.

\textbf{ARJudge} introduces a robust, multi-faceted evaluation framework that combines adaptive criteria generation with both text-based and code-driven analysis to assess LLM outputs.
Fine-tuned on the Composite Analysis Corpus, ARJudge achieves strong performance across benchmarks like PandaLM Eval and MTBench, with an average accuracy of 77.7\% and even outperforming tuning-free models like Qwen2.5-7B by up to 26.7\% on challenging datasets such as LLMBar.
However, ARJudge still relies on a moderately large model (Qwen2.5-7B) and incurs non-trivial fine-tuning costs, limiting its accessibility for lightweight deployment.
Additionally, its performance gains rely heavily on code execution capabilities, which may not be feasible in constrained environments\cite{ar_judge}.


\textbf{LLM Cascades with Mixture of Thought Representations} introduce a cost-efficient evaluation framework that strategically combines weak and strong LLMs using answer consistency as a routing signal.
The system leverages weaker models like GPT-3.5 to produce multiple intermediate reasoning traces—such as Chain-of-Thought (CoT) and Program-of-Thought (PoT)—and accepts their answers when the outputs are sufficiently consistent.
Experiments on six reasoning benchmarks show that this method maintains accuracy close to full GPT-4 evaluation while reducing total cost by up to 60\%.
However, the approach assumes that consistency among weaker LLMs correlates with correctness, which may not always hold in complex or adversarial tasks.
Moreover, all the experiments were aimed to testing the capabilities of the system, but not in terms of LLM-as-a-judge \cite{yue2024largelanguagemodelcascades}.

\subsection{Final Comparison}\label{subsec:final-comparison}

Table~\ref{tab:bias-comparison} summarizes the key characteristics of the reviewed approaches with respect to the bias they address, their core methodology, and notable limitations.


\begin{table}[H]
\small
\centering
\caption{Comparison of mitigation approaches across evaluation biases}
\label{tab:bias-comparison}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|p{2.6cm}|p{1.6cm}|p{3.0cm}|p{4.2cm}|}
\hline
\textbf{Approach} & \textbf{Bias Targeted} & \textbf{Core Technique} & \textbf{Limitations} \\
\hline
PORTIA & Position & Input alignment with merged prompt & Not robust when LLM refuses judgment; no codebase; limited by context window \\
\hline
FairEval & Position & Score calibration using contextual references & High API usage; partial human dependency; limited scalability \\
\hline
LC AlpacaEval & Length & Logistic regression debiasing verbosity artifacts & Weaker on multi-turn / open-ended tasks; static model, handcrafted features \\
\hline
CascadeEval & Cost & Cascaded abstention with fallback to strong models & Needs careful confidence threshold tuning; benchmark-dependent \\
\hline
ARJudge & Cost & Code-based adaptive prompting & Fine-tuning and larger model needed; less cost-efficient and portable \\
\hline
LLM Cascades + MoT & Cost & Consistency check across CoT/PoT reasoning & Assumes agreement = correctness; may fail on adversarial examples \\
\hline
\end{tabular}%
}
\end{table}

As observed, current solutions are optimized for specific bias types but fall short of offering a unified remedy.
PORTIA and SOD address position bias but suffer from deployment or scalability limitations.
LC AlpacaEval effectively mitigates length bias but lacks adaptability for multi-turn scenarios.
CascadeEval emerges as a practical framework for reducing evaluation cost with minimal performance degradation but does not address other biases.

Given these insights, the goal of the present study is to design a new evaluation framework that addresses all three issues—position bias, length bias, and cost—by integrating the core strengths of \textbf{CascadeEval}, \textbf{FairEval}, and \textbf{LC AlpacaEval}.


\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}