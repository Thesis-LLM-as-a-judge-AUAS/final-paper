\documentclass[sigconf, authoryear]{acmart}

\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{titlesec}


\titlespacing*{\subsubsection}{0pt}{*0}{1ex}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
    \providecommand\BibTeX{{%
        Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
conference title from your rights confirmation email}{June 03--05,
    2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{LLM-as-a-judge}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Mokrov Semen}
\affiliation{%
    \institution{Amsterdam University of Applied Science}
    \city{Amsterdam}
    \country{The Netherlands}}
\email{semen.mokrov@gmail.com}

\author{Deep Kayal}
\affiliation{%
    \institution{Amazon Research}
    \city{London}
    \country{The United Kingdom}}
\email{_}

\author{Riccardo Pinosio}
\affiliation{%
    \institution{Amsterdam University of Applied Science}
    \city{Amsterdam}
    \country{The Netherlands}}
\email{_}

\author{Debarati Bhaumik}
\affiliation{%
    \institution{Amsterdam University of Applied Science}
    \city{Amsterdam}
    \country{The Netherlands}}
\email{_}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Mokrov et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    Here the abstract
\end{abstract}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}


Over the past five years, the surge in released Large Language Models (LLMs) has gathered enormous attention of the research community.
To address the challenge of assessing the quality of these new models, three types of benchmarks have been introduced:
\begin{itemize}
    \item \textit{Human judgement} - the estimation based on the historical human scores and decisions\cite{chiang2024chatbotarenaopenplatform,liu2025amelostableframeworkarenabased};
    \item \textit{Ground-truth} - the estimation based on the alignment with the factual ground-truth\cite{hendrycks2021measuringmassivemultitasklanguage,srivastava2023beyond,zeng2024mrgsm8kmetareasoningbenchmarklarge,myrzakhan2024openllmleaderboardmultichoiceopenstylequestions};
    \item \textit{LLM-as-a-judge} - the estimation based on the decision of another Large Language Model\cite{mtbench,alpaca_eval,zhu2025judgelmfinetunedlargelanguage};
\end{itemize}

While all the approaches are legitimate, \textbf{LLM-as-a-judge} techniques show the strongest potential, as they are substantially more cost-effective and rapid than analogs that involve human's evaluation \cite{salinas2025tuningllmjudgedesign}.
Apart from that, these methods are capable of assessing the answers without predefined answers in contrast to the ground-truth frameworks \cite{zhu2025judgelmfinetunedlargelanguage}.
Nevertheless, the pure LLM-as-a-judge approach and current frameworks still fractionally suffer from distinct sort of biases such as \textit{position} \cite{shi2024judging, li-etal-2024-split, wei2025systematicevaluationllmasajudgellm}, \textit{length} \cite{hu2024explaining, lc_alpacha}, \textit{self-preference} \cite{self-preference}, \textit{transitivity} \cite{xu2025investigatingnontransitivityllmasajudge}, etc.
Moreover, current systems that relies on state-of-art-model are showing \textit{high expenses} during the evaluation process \cite{cascade_eval,ar_judge,salinas2025tuningllmjudgedesign,yue2024largelanguagemodelcascades}.

This paper is primarily concerned on comparing and integrating mitigation strategies for the following biases and problems within the scope of pairwise comparison:
\begin{itemize}
    \item \textit{Position bias} - the tendency of a judge to favor responses based on their position in the prompt regardless the content quality;
    \item \textit{Length bias} - the proneness of the judge to evaluate longer responses higher without dependence on their quality;
    \item \textit{Cost-efficiency} - since the most accurate judgments are produced by state-of-the-art models, the resulting API cost may represent a major obstacle to scalability.
\end{itemize}

The new approach for mitigation the described biases called \textbf{SemenEval} was introduced.
This approach builds upon and integrates several observed techniques:
\begin{itemize}
    \item \textit{FairEval} - mitigates position bias in LLM evaluations through adjusting the model prompting and mixing the order of judged answers \cite{faireval};
    \item \textit{CascadeEval} - combines fine-tuned judges with GPT-4 using developed confidence-based score to balance cost and evaluation accuracy \cite{cascade_eval};
    \item \textit{LC AlpacaEval} - debiases evaluations by controlling for length differences using pre-trained logistic regressions with its own loss function \cite{lc_alpacha};
\end{itemize}

It leverages the strengths of each approach to perform unbiased evaluations based on the \textit{LC Alpaca Win Rate}, while incorporating the cost-effective strategy for mitigating position bias \cite{alpaca_eval, lc_alpacha}.


%The main contributions of this paper are following:
%\begin{itemize}
%    \item
%\end{itemize}


\section{Related Work}\label{sec:related-work}

\subsection{Large Language Models as Evaluators}\label{subsec:large-language-models-as-evaluators}

\subsubsection{Introduction to LLM Evaluators}

Large Language Models (LLMs) are increasingly being used as automated evaluators for open-ended text generation tasks, providing a scalable alternative to slow and costly human judgment.
Recent benchmarks and leaderboards for chatbots and text generation often rely on powerful LLMs (such as GPT-4) to rank or score model outputs in place of humans.
For instance, the Vicuna Chatbot evaluation pipeline employs GPT-4 to compare responses in a pairwise manner, achieving evaluation agreements on par with human annotators on many queries.
Researchers have even built interactive frameworks like \textit{EvalGen} \cite{evalgen} that leverage LLMs to suggest evaluation criteria and generate test queries for model outputs, with a human-in-the-loop to refine these criteria.
These approaches demonstrated the potential of LLM-as-a-judge systems: a strong LLM can approximate human evaluation reasonably well (e.g., GPT-4’s judgments agree with human preferences roughly 75–80\% of the time on multi-turn dialog tasks) \cite{mtbench}.
As a result, LLM evaluators have quickly become a central tool for comparing models and tuning them to human-like performance.

\subsubsection{Evaluation Paradigms}

LLM-as-a-Judge evaluations mostly employ two paradigms: pointwise and pairwise evaluations.

\paragraph{Pointwise Evaluation}
In pointwise evaluation, an LLM assesses a single response independently, assigning a score based on predefined criteria such as relevance, coherence, or factual accuracy.
This method is straightforward and allows for fine-grained analysis of individual responses.
However, it may suffer from inconsistencies due to the subjective nature of scoring and the lack of comparative context \cite{evalgen}.

\paragraph{Pairwise Evaluation}
Pairwise evaluation involves presenting the LLM with two responses to the same prompt and asking it to determine which one is better according to specific criteria \cite{liusie2024llmcomparativeassessmentzeroshot}.
This comparative approach can mitigate some biases inherent in pointwise evaluations and often aligns more closely with human judgment.
In score-based pairwise comparison, the LLM assigns each response a numerical score based on evaluation criteria such as coherence, helpfulness, or relevance, and then ranks them by comparing the scores \cite{liu2024aligning}.
This method allows for fine-grained differentiation between responses while retaining the benefits of comparative judgment.

\subsubsection{Benchmarks Utilizing LLM-as-a-Judge}


Several benchmarks have been developed to standardize the evaluation of LLM outputs using the LLM-as-a-Judge paradigm.

\paragraph{MT-Bench}


MT-Bench is a multi-turn benchmark designed to assess the conversational abilities of LLMs.
It evaluates models on their capacity to maintain context, follow instructions, and provide informative responses over multiple dialogue turns.
MT-Bench employs GPT-4 as the judge, leveraging its advanced understanding to score model responses.
The benchmark includes an Elo rating system derived from pairwise comparisons, offering a dynamic leaderboard that reflects the relative performance of various models\cite{mtbench}.

\paragraph{AlpacaEval}


AlpacaEval focuses on single-turn instruction-following tasks, evaluating models based on their ability to generate helpful and accurate responses.
It uses GPT-4 as an automatic evaluator to compare model outputs against reference responses.
AlpacaEval provides win-rate statistics and supports a leaderboard that ranks models accordingly.
The benchmark has been validated against a substantial set of human annotations, ensuring its reliability and relevance\cite{alpaca_eval}.

\paragraph{JudgeLM}
JudgeLM is a framework that is focused on the judging the ability of LLM to response in open-ended scenarios.
It leverages pre-fine-tuned models such as Vicuna under the hood as a judge.
This framework partially supports existing bias-mitigation algorithms such as \textit{swap augmentation} or \textit{reference support}.
Additionally, it also supports \textit{multimodal inputs} and is capable of evaluating \textit{multi-turn dialogs}.

\subsection{Problems in Current Systems}\label{subsec:problems-in-current-systems}

\subsubsection{Position Bias}

Position bias refers to the tendency of LLM-based evaluators to favor responses based on their position in a prompt during pairwise comparisons.
Studies have shown that models like GPT-4 often prefer the first response presented, regardless of content quality\cite{shi2024judging}.
This bias can compromise the fairness and reliability of evaluations.
To address this, frameworks such as PORTIA have been developed, which align similar content across candidate answers to mitigate position bias effectively\cite{li-etal-2024-split}.

\subsubsection{Length Bias}

Length bias occurs when LLM evaluators disproportionately favor longer responses, associating verbosity with higher quality.
This can lead to inflated evaluations for unnecessarily lengthy outputs.
Research has demonstrated that longer responses often receive higher preference scores, even when shorter responses are equally or more informative\cite{hu2024explaining}.

\subsubsection{Cost Problem}

Employing state-of-the-art large language models (LLMs) as evaluators in LLM-as-a-Judge frameworks introduces significant computational costs.
High-performance models like GPT-4 entail substantial expenses per token, rendering large-scale evaluations financially heavy.
Simpler queries are handled by smaller, cost-effective models, while more complex tasks are escalated to larger, more expensive models.
This strategy can significantly reduce overall evaluation costs without compromising accuracy\cite{chen2023frugalgpt}.
At the same time, ARJudge optimizes the trade-off between evaluation quality and computational expense by dynamically allocating resources based on the prompt characteristics\cite{ar_judge}.

\subsection{Established Techniques for Biases Mitigation}\label{subsec:established-techniques-for-bias-mitigation}

This chapter presents established techniques for mitigating position bias, length bias, and cost-related issues.

\textbf{PORTIA} (Position-Oriented Response Transformation and Input Alignment) is an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner.
Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs.
Experiments demonstrate that PORTIA markedly enhances the consistency rates for various models and comparison forms, achieving an average relative improvement of 47.46\%.
Furthermore, it rectifies around 80\% of the position bias instances within the GPT-4 model, elevating its consistency rate up to 98\%.
However, PORTIA is not effective when the LLM evaluator refuses to provide a judgment, often seen in advanced models like GPT-3.5 and GPT-4 in sensitive domains (e.g., roleplay tasks).
Additionally, it requires the merged input to fit within the model’s context window, and it's not reproducible since it doesn't provide a working codebase at the time of writing\cite{portia}.

\textbf{FairEval} introduces a calibration framework to address positional bias in LLM evaluations.
Extensive experiments demonstrate that FairEval successfully alleviates evaluation bias, achieving a closer alignment with human judgment by improving consistency and reducing positional artifacts in comparative outputs.
Specifically, the combined calibration strategy using all the components enhances the accuracy of GPT-4 and ChatGPT’s evaluations by up to 14.3\% and 26.9\%, respectively, with a kappa correlation improvement of 0.25 and 0.46.
However, despite its effectiveness, FairEval's reliance on multiple API calls and partial human intervention may lead to increased computational costs and scalability limitations when applied to large-scale model comparisons\cite{faireval}.

\textbf{SOD} is a framework for mitigating position bias in LLMs in general without requiring human-labeled data.
It generates unsupervised responses using diverse prompting strategies, filters them, and optimizes a joint loss combining task and debiasing objectives.
Experiments across eight datasets and five tasks show that SOD outperforms existing baselines in reducing declared biases.
For instance, it improves non-biased test performance by up to 2\% in ROUGE-L\@.
Despite effectiveness, SOD is only partially related to the LLM-as-a-judge task, as it targets position bias in generation rather than evaluating responses through pairwise comparison\cite{liu2024selfsupervisedpositiondebiasinglarge}.

\textbf{LLM-JudgeEval} introduces a systematic framework for evaluating the reliability and bias of LLM-as-a-Judge methods by defining theoretically grounded metrics for accuracy, position bias, and length bias, and mitigating self-inconsistency.
The authors propose interpretable formulations of position and length biases and quantify internal inconsistencies through a flipping noise model.
However, while the study defines metrics to measure position and length bias, it does not propose any method to mitigate these biases, instead emphasizing their presence.
Thus, this approach lies between a benchmark and a bias-solving solution\cite{wei2025systematicevaluationllmasajudgellm}.

\textbf{G-EVAL} proposes a novel evaluation framework that leverages GPT-4 in a form-filling paradigm with chain-of-thought prompting to assess NLG outputs on different dimensions.
The system auto-generates step-by-step evaluation instructions and computes continuous quality scores by reweighting discrete ratings with token-level output probabilities.
G-EVAL-4 achieves the highest human correlation across all benchmarks, with a Spearman correlation of \textit{0.514} on SummEval and \textit{0.599} on QAGS, outperforming state-of-the-art metrics such as UniEval and GPTScore.
However, it reveals a systematic bias in favor of self-generated outputs\cite{liu2023gevalnlgevaluationusing}.

\textbf{ZEPO} is a meta-evaluation framework designed to improve human alignment in LLM-as-a-judge evaluations by directly targeting preference bias, including position bias, through zero-shot prompt optimization.
The authors did not limit their focus to the LLM-as-a-judge problem; rather, the proposed framework is designed to enhance the evaluation capabilities of LLMs across all types of responses, including those generated by humans.
Rather than relying on labeled data or manual prompt design, ZEPO formulates a zero-shot fairness objective based on the uniform distribution of LLM preferences.
It uses an LLM-based optimizer to iteratively paraphrase prompts, searching for those that yield the fairest (most balanced) decisions.
Experimental results show that ZEPO achieves up to 29\% relative gains in Spearman correlation with human judgment over strong pairwise baselines in terms of coherence, relevancy, informativeness, etc.
When combined with position debias (e.g., Balanced Position Calibration\cite{wang2023largelanguagemodelsfair}), ZEPO further improves performance, achieving up to 8\% improvement in Spearman correlation comparing to the pairwise comparison combined with the debias technique.
However, the current implementation uses a basic greedy search with GPT-3.5 for prompt optimization, leaving room for performance gains through more advanced search techniques and optimizer models\cite{zhou2024zepo}.

\textbf{Bias-Mitigation} proposes two complementary strategies to mitigate evaluation bias in LLM-based judges: online calibration for closed-source models and offline contrastive training for open-source ones.
The calibration method subtracts estimated superficial quality from predicted scores using either \textit{log-probabilities} or \textit{prompt-based generation}.
The \textit{contrastive training} approach involves fine-tuning the model on instruction–answer–counteranswer triplets, where the counteranswers are missing the instruction, but well-written.
Evaluated on the LLMBar benchmark, the calibrated GPT-4 model improved accuracy on adversarial sets (e.g., GPTOut: 79.8\% $\rightarrow$ 86.2\%).
At the same time, contrastive training with Vicuna-7B raised average performance from 39.7\% to 57.2\%.
Although, despite position and length biases are defined as a part of superficial bias, the paper does not specifically focus on mitigating either of them\cite{zhou2024mitigatingbiaslargelanguage}.

\textbf{LC AlpacaEval} addresses length bias issue by training a logistic regression model on prior annotations from AlpacaEval, factoring in model identity, prompt length, and instruction difficulty.
By neutralizing the response length component during score prediction, this method improves evaluation fairness.
As a result, the automatic evaluation match human opinions better (raising the Spearman correlation with Chatbot Arena from 0.94 to 0.98).
Moreover, their system is more robust to the fluctuations in the win rate: before, models could change their win rate by as much as 41.4 percentage points just by changing how long their answers were; now, it is 9.7.
Also, during adversarial attack (attempt to deceive the system by purpose), the win rate only goes up by 8.5 points, instead of 22.2.
However, LC AlpacaEval's performance is bounded by the scope of its training data; it struggles to generalize effectively to more complex multi-turn tasks and open-ended domains, where human judgment nuances are more difficult to approximate\cite{lc_alpacha}.

\textbf{AdapAlpaca} tackles the length bias problem by adjusting the evaluation setup itself rather than correcting scores post hoc.
Specifically, it ensures that each model response is compared to a reference response of similar length, by dividing the word count range into intervals (e.g., 0–200, 200–400 words) and prompting GPT-4 to generate responses within those ranges.
This approach works under the assumption that the final response quality that a judge assign to an answer consist of two elements: \textbf{desirability} (length-independent) and \textbf{information mass} (length-dependent).
The presented benchmark shows that the average difference between its win rates and human preferences is just \textit{0.99\%}, compared to \textit{+24.35\%} for concise responses and \textit{+4.22\%} for detailed responses under the standard AlpacaEval metric.
However, its reliance on fixed-length generation assumes that quality can be preserved across word counts.
Moreover, the method still depends on GPT-4 for reference generation and annotation\cite{hu2024explaininglengthbiasllmbased}.

\textbf{Tuned LLM-judges} introduces a multi-fidelity, multi-objective framework to systematically optimize hyperparameters of zero-shot LLM judges, including \textit{model choice}, \textit{prompt design}, \textit{output format}, and \textit{inference settings}.
4480 judge configurations were evaluated using open-weight models and progressively filtered through a three-stage evaluation, reducing costs by leveraging \textbf{human agreement} as a cheaper yet effective metric.
Their best-performing judge achieves a human agreement score of \textit{0.49} at the lower cost on the LMSys dataset, outperforming JudgeLM and PandaLM, and corresponding Arena-Hard’s accuracy.
On the Arena-Hard benchmark, the tuned judge achieves a Spearman correlation of \textit{0.93}, surpassing even GPT-4 and Claude-based judges in ranking alignment.
Although, the introduced method still illustrates LLM judge limitations such as prompt sensitivity, stylistic bias, and variability across model sizes\cite{salinas2025tuningllmjudgedesign}.

\textbf{CascadeEval} proposes a cascade-based evaluation mechanism with early abstention, allowing smaller, cheaper LLMs to abstain from answering difficult queries and defer them to stronger models.
This leads to a 13.0\% average reduction in cost and a 5.0\% drop in error rates across six benchmarks, with only a 4.1\% increase in abstention rates, demonstrating strong performance-cost trade-offs.
However, the method requires careful calibration of confidence thresholds and may depend on benchmark-specific tuning\cite{cascade_eval}.

\textbf{ARJudge} introduces a robust, multi-faceted evaluation framework that combines adaptive criteria generation with both text-based and code-driven analysis to assess LLM outputs.
Fine-tuned on the Composite Analysis Corpus, ARJudge achieves strong performance across benchmarks like PandaLM Eval and MTBench, with an average accuracy of 77.7\% and even outperforming tuning-free models like Qwen2.5-7B by up to 26.7\% on challenging datasets such as LLMBar.
However, ARJudge still relies on a moderately large model (Qwen2.5-7B) and incurs non-trivial fine-tuning costs, limiting its accessibility for lightweight deployment.
Additionally, its performance gains rely heavily on code execution capabilities, which may not be feasible in constrained environments\cite{ar_judge}.

\textbf{LLM Cascades with Mixture of Thought Representations} introduce a cost-efficient evaluation framework that strategically combines weak and strong LLMs using answer consistency as a routing signal.
The system leverages weaker models like GPT-3.5 to produce multiple intermediate reasoning traces—such as Chain-of-Thought (CoT) and Program-of-Thought (PoT)—and accepts their answers when the outputs are sufficiently consistent.
Experiments on six reasoning benchmarks show that this method maintains accuracy close to full GPT-4 evaluation while reducing total cost by up to 60\%.
However, the approach assumes that consistency among weaker LLMs correlates with correctness, which may not always hold in complex or adversarial tasks.
Moreover, all the experiments were aimed to testing the capabilities of the system, but not in terms of LLM-as-a-judge \cite{yue2024largelanguagemodelcascades}.

\subsection{Final Comparison}\label{subsec:final-comparison}

Table~\ref{tab:bias-comparison} summarizes the key characteristics of the reviewed approaches with respect to the bias they address, their core methodology, and notable limitations.
\begin{table}[H]
    \small
    \centering
    \caption{Comparison of mitigation approaches across evaluation biases}
    \label{tab:bias-comparison}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{|p{2.6cm}|p{1.6cm}|p{3.0cm}|p{4.2cm}|}
            \hline
            \textbf{Approach}  & \textbf{Mitigation Target}       & \textbf{Core Technique}                                                         & \textbf{Limitations}                                                                        \\
            \hline
            PORTIA             & Position bias                    & Input alignment with merged prompt                                              & Not robust when LLM refuses judgment; no codebase; limited by context window                \\
            \hline
            FairEval           & Position bias                    & Score calibration using contextual references                                   & High API usage; partial human dependency; limited scalability                               \\
            \hline
            SOD                & Position bias                    & Self-supervised debiasing via generating unsupervised responses and fine-tuning & The approach isn't related to LLM-as-a-judge pairwise comparison                            \\
            \hline
            LLM-JudgeEval      & Position bias \& length bias     & Definitions of accuracy and bias metrics and their calculations                 & The mitigation approach isn't introduced; the framework only emphasis the biases            \\
            \hline
            ZEPO               & Not specified                    & Zero-shot prompt optimization for fairness-based preference calibration         & Basic optimizer limits performance gains                                                    \\
            \hline
            G-Eval             & Not specified                    & Usage of chain-of-thought prompting and a form-filling paradigm                 & Favor bias of self-generated outputs                                                        \\
            \hline
            Bias-Mitigation    & Not specified (superficial bias) & Bias calibration with instruction-tuned model or fine-tuning                    & Not specified on the position and length biases problems (incomparable                      \\
            \hline
            LC AlpacaEval      & Length bias                      & Logistic regression debiasing verbosity artifacts                               & Annotator dependency; the same quality assumption                                           \\
            \hline
            AdapAlpaca         & Length bias                      & Match the model's reply with the similar size reference                         & Fixed-length responses must preserve quality, relies on the state-of-art-model as annotator \\
            \hline
            CascadeEval        & Cost problem                     & Cascaded abstention with fallback to strong models                              & Needs careful confidence threshold tuning; benchmark-dependent                              \\
            \hline
            Tuned LLM-judges   & Cost problem                     & Multi-fidelity search over 4,480 judge configurations using human-agreement     & Prompt sensitivity, stylistic bias, model sizes variability                                 \\
            \hline
            ARJudge            & Cost problem                     & Code-based adaptive prompting                                                   & Fine-tuning and larger model needed; less cost-efficient and portable                       \\
            \hline
            LLM Cascades + MoT & Cost problem                     & Consistency check across CoT/PoT reasoning                                      & Assumes agreement = correctness; may fail on adversarial examples                           \\
            \hline
        \end{tabular}%
    }
\end{table}

As observed, current solutions are optimized for specific bias types but fall short of offering a unified remedy.
PORTIA and SOD address position bias but suffer from deployment or scalability limitations.
LC AlpacaEval effectively mitigates length bias but lacks adaptability for multi-turn scenarios.
CascadeEval emerges as a practical framework for reducing evaluation cost with minimal performance degradation but does not address other biases.

Given these insights, the goal of the present study is to design a new evaluation framework that addresses all three issues—position bias, length bias, and cost—by integrating the core strengths of \textbf{CascadeEval}, \textbf{FairEval}, and \textbf{LC AlpacaEval}.


\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}