@misc{evalgen,
    title = {Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences},
    author = {Shreya Shankar and J. D. Zamfirescu-Pereira and Björn Hartmann and Aditya G. Parameswaran and Ian Arawjo},
    year = {2024},
    eprint = {2404.12272},
    archivePrefix = {arXiv},
    primaryClass = {cs.HC},
    url = {https://arxiv.org/abs/2404.12272},
}

@misc{mtbench,
    title = {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
    author = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
    year = {2023},
    eprint = {2306.05685},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2306.05685},
}

@misc{alpaca_eval,
    author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
    title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
    year = {2023},
    month = {5},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@misc{liusie2024llmcomparativeassessmentzeroshot,
    title = {LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models},
    author = {Adian Liusie and Potsawee Manakul and Mark J. F. Gales},
    year = {2024},
    eprint = {2307.07889},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2307.07889},
}

@inproceedings{
liu2024aligning,
    title = {Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators},
    author = {Yinhong Liu and Han Zhou and Zhijiang Guo and Ehsan Shareghi and Ivan Vuli{\'c} and Anna Korhonen and Nigel Collier},
    booktitle = {First Conference on Language Modeling},
    year = {2024},
    url = {https://openreview.net/forum?id=9gdZI7c6yr}
}

@misc{shi2024judging,
    title = {Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge},
    author = {Lin Shi and Chiyu Ma and Wenhua Liang and Xingjian Diao and Weicheng Ma and Soroush Vosoughi},
    year = {2025},
    eprint = {2406.07791},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2406.07791},
}

@misc{hu2024explaining,
    title = {Explaining Length Bias in LLM-Based Preference Evaluations},
    author = {Zhengyu Hu and Linxin Song and Jieyu Zhang and Zheyuan Xiao and Tianfu Wang and Zhengyu Chen and Nicholas Jing Yuan and Jianxun Lian and Kaize Ding and Hui Xiong},
    year = {2024},
    eprint = {2407.01085},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2407.01085},
}

@misc{chen2023frugalgpt,
      title={FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance},
      author={Lingjiao Chen and Matei Zaharia and James Zou},
      year={2023},
      eprint={2305.05176},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.05176},
}

@misc{cascade_eval,
      title={An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4},
      author={Hui Huang and Yingqi Qu and Xingyuan Bu and Hongli Zhou and Jing Liu and Muyun Yang and Bing Xu and Tiejun Zhao},
      year={2024},
      eprint={2403.02839},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.02839},
}

@misc{ar_judge,
      title={Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework},
      author={Kaishuai Xu and Tiezheng Yu and Wenjun Hou and Yi Cheng and Liangyou Li and Xin Jiang and Lifeng Shang and Qun Liu and Wenjie Li},
      year={2025},
      eprint={2502.18874},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18874},
}

@inproceedings{li-etal-2024-split,
    title = "Split and Merge: Aligning Position Biases in {LLM}-based Evaluators",
    author = "Li, Zongjie  and
      Wang, Chaozheng  and
      Ma, Pingchuan  and
      Wu, Daoyuan  and
      Wang, Shuai  and
      Gao, Cuiyun  and
      Liu, Yang",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.621/",
    doi = "10.18653/v1/2024.emnlp-main.621",
    pages = "11084--11108",
    abstract = "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, taking into account both length and semantics, and merges them back into a single prompt for evaluation by LLMs. Extensive experiments with six LLMs on 11,520 answer pairs demonstrate that PORTIA markedly enhances the consistency rates for all models and forms of comparison tested, achieving an average relative improvement of 47.46{\%}. It also enables PORTIA-enhanced GPT-3.5 to achieve agreement rates with humans comparable to GPT-4 and elevates GPT-4`s consistency rate up to 98{\%}. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass standalone GPT-4 in terms of alignment with human evaluators, highlighting PORTIA`s ability to correct position bias, improve LLM consistency, and boost performance while keeping cost efficiency."
}

@misc{portia,
      title={Split and Merge: Aligning Position Biases in LLM-based Evaluators},
      author={Zongjie Li and Chaozheng Wang and Pingchuan Ma and Daoyuan Wu and Shuai Wang and Cuiyun Gao and Yang Liu},
      year={2024},
      eprint={2310.01432},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.01432},
}

@misc{faireval,
      title={Large Language Models are not Fair Evaluators},
      author={Peiyi Wang and Lei Li and Liang Chen and Zefan Cai and Dawei Zhu and Binghuai Lin and Yunbo Cao and Qi Liu and Tianyu Liu and Zhifang Sui},
      year={2023},
      eprint={2305.17926},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.17926},
}

@misc{yue2024largelanguagemodelcascades,
      title={Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning},
      author={Murong Yue and Jie Zhao and Min Zhang and Liang Du and Ziyu Yao},
      year={2024},
      eprint={2310.03094},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.03094},
}

@misc{lc_alpacha,
      title={Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators},
      author={Yann Dubois and Balázs Galambosi and Percy Liang and Tatsunori B. Hashimoto},
      year={2025},
      eprint={2404.04475},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.04475},
}

@misc{zhou2024zepo,
      title={Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments},
      author={Han Zhou and Xingchen Wan and Yinhong Liu and Nigel Collier and Ivan Vulić and Anna Korhonen},
      year={2024},
      eprint={2406.11370},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11370},
}